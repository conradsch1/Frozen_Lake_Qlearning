import numpy as np
import gym
import random
import csv
import time
from IPython.display import clear_output

#global vars
num_episodes = 10000
max_steps_per_episode = 100

discount_rate = 0.99
learning_rate = 0.002

exploration_rate = 1
max_exploration_rate = 1
min_exploration_rate = 0.01
exploration_decay_rate = 0.001

formated_data = []

#loop
while learning_rate <= 0.95 :
    env = gym.make("FrozenLake-v0")

    action_space_size = env.action_space.n
    state_space_size = env.observation_space.n

    q_table = np.zeros((state_space_size, action_space_size))
    #print(q_table)

    ##vid 2
    rewards_all_episodes = []

    #Q-learning algo
    for episode in range(num_episodes):
        state = env.reset()

        done = False
        rewards_current_episode = 0

        for step in range(max_steps_per_episode):

            #exploration-exploitation trade-off
            exploration_rate_thershold = random.uniform(0, 1)
            if exploration_rate_thershold > exploration_rate:
                action = np.argmax(q_table[state,:])
            else :
                action = env.action_space.sample()

            new_state, reward, done, info = env.step(action)

            #Update Q-table for Q(state,action)
            q_table[state, action] = q_table[state, action] * (1-learning_rate) + learning_rate *(reward + discount_rate * np.max(q_table[new_state, :]))

            state = new_state
            rewards_current_episode += reward

            if done == True :
                break

        #Exploration rate decay
        exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate ) * np.exp(-exploration_decay_rate*episode)

        rewards_all_episodes.append(rewards_current_episode)

    # calculate and print the average reward per thousand episodes
    rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes/1000)
    count = 1000

    #print("*******Average reward per thousand episodes\n")
    #for r in rewards_per_thousand_episodes:
       # print(count, ": ", str(sum(r/1000)))
        #count += 1000

    episode = []
    rewards_per_thousand_episodes_data = []
    rewards_per_thousand_episodes_max = []


    #write data sets
    for r in rewards_per_thousand_episodes:
        rewards_per_thousand_episodes_data.append(sum(r/1000))
        episode.append(count)
        count += 1000



    #print updated Q-table
    #print("\n\n***************Q-table*****\n")
    #print(q_table)



    ##csv writing
    import csv

    # csv header
    header = ['rewards_per_thousand_episodes_max', 'learning_rate']


    # csv data

    dict = {"rewards_per_thousand_episodes_max": max(rewards_per_thousand_episodes_data),
                "learning_rate": learning_rate}
    formated_data.append(dict)

    if learning_rate <= 0.027:
        learning_rate = learning_rate + 0.001

    elif learning_rate <= 0.15:
        learning_rate = learning_rate + 0.007

    else:
       learning_rate = learning_rate + 0.01

    print(learning_rate)
    print(dict)


#write csv file

with open('trail1.csv', 'w') as f:
    writer = csv.DictWriter(f, fieldnames= header)
    writer.writeheader()
    writer.writerows(formated_data)


# watch
#for episode in range(3):
    #state = env.reset()
    #done = False
    #print("*******EPISODE", episode +1, "*******\n\n\n\n")
    #time.sleep(1)

    #for step in range(max_steps_per_episode):
        #clear_output(wait = True)
        #env.render()
        #time.sleep(0.3)

        #action = np.argmax(q_table[state, :])
        #new_state, reward, done, info = env.step(action)

        #if done:
            #clear_output(wait = True)
            #env.render()
            #if reward == 1:
                #print("********You fell in a hole********")
                #time.sleep(3)

            #else:
                #print("*****You reached the goal******")
                #time.sleep(3)

            #clear_output(wait = True)
            #break

        #state = new_state

env.close()
